{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The `Layer` class: the combination of state (weights) and some computation\n",
    "- A layer encapsulates both **a state** (the layer's \"weights\") and **a transform** from inputs to outputs (a \"call\", the layer's forward pass).\n",
    "- Here's a densely-connected linear layer.\n",
    "    - It has a state: the variables `w` and `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        \n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value = w_init(shape=(input_dim, units), dtype='float32'),\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value=b_init(shape=(units,), dtype='float32'),\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You would use a layer by calling it on some tensor input(s), much like a Python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=28, shape=(2, 4), dtype=float32, numpy=\n",
       "array([[ 0.05453006,  0.03039527, -0.0578984 , -0.07564189],\n",
       "       [ 0.05453006,  0.03039527, -0.0578984 , -0.07564189]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.ones((2,2))\n",
    "linear_layer = Linear(4, 2)\n",
    "y = linear_layer(X)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that the weights `w` and `b` are automatically tracked by the layer upon being set as layer attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that a quicker shortcut for adding weight to a layer is the **`add_weight` method**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        \n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_dim, units),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        self.b = self.add_weight(\n",
    "            shape=(units,),\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=56, shape=(2, 4), dtype=float32, numpy=\n",
       "array([[ 0.09064088, -0.10099248,  0.07526717, -0.03107435],\n",
       "       [ 0.09064088, -0.10099248,  0.07526717, -0.03107435]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.ones((2,2))\n",
    "linear_layer = Linear(4, 2)\n",
    "y = linear_layer(X)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers can have non-trianable weights\n",
    "- Besides trainable weights, you can add non-trainable weights to a layer as well.\n",
    "- Such weights are meant not to be taken into account during backpropagation, when the layer is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeSum(keras.layers.Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),\n",
    "                                trainable=False)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1.]\n",
      " [1. 1.]], shape=(2, 2), dtype=float32)\n",
      "[2. 2.]\n",
      "[4. 4.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones((2, 2))\n",
    "my_sum = ComputeSum(2)\n",
    "\n",
    "print(X)\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best practice: deferring weight creation until the shape of the inputs is known\n",
    "- Our `Linear` layer above took an `input_dim` argument in `__init__()`, which was used to compute the shape of the weights `w` and `b`.\n",
    "- However, in many cases, you may not know in advance the size of your inputs, and you would like to create weights when that value becomes known, some time after instantiating the layer.\n",
    "- In the Keras API, we (TensorFlow team) recommend **creating layer weights in the `build(self, inputs_shape)` method** of the layer.\n",
    "    - The `__call__()` method of the layer will automatically run `build()` the first time it is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous version\n",
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        \n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_dim, units),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        self.b = self.add_weight(\n",
    "            shape=(units,),\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practice\n",
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        self.b = self.add_weight(\n",
    "            shape = (self.units,),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=102, shape=(2, 30), dtype=float32, numpy=\n",
       "array([[-0.06031368,  0.05893853,  0.01399888, -0.08956892,  0.11585691,\n",
       "        -0.06668353,  0.05894025, -0.03837348,  0.20267746,  0.03161312,\n",
       "         0.04975317, -0.01819842, -0.0004446 , -0.09089342, -0.03513056,\n",
       "         0.0654503 ,  0.02855569,  0.02434471, -0.05429384, -0.01382153,\n",
       "        -0.00905077,  0.02210669,  0.04630881,  0.0711771 , -0.04193326,\n",
       "         0.03831274, -0.05932758,  0.12649518, -0.0847965 ,  0.07688626],\n",
       "       [-0.06031368,  0.05893853,  0.01399888, -0.08956892,  0.11585691,\n",
       "        -0.06668353,  0.05894025, -0.03837348,  0.20267746,  0.03161312,\n",
       "         0.04975317, -0.01819842, -0.0004446 , -0.09089342, -0.03513056,\n",
       "         0.0654503 ,  0.02855569,  0.02434471, -0.05429384, -0.01382153,\n",
       "        -0.00905077,  0.02210669,  0.04630881,  0.0711771 , -0.04193326,\n",
       "         0.03831274, -0.05932758,  0.12649518, -0.0847965 ,  0.07688626]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer = Linear(30)\n",
    "y = linear_layer(X)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers are recursively composable\n",
    "- If you assign a `Layer` instance as attribute of another layer, the outer layer will start tracking the weights of the inner layer.\n",
    "- We recommend **creating sublayers in the `__init__()` method** since the sublayers will typically have a `build()` method, they will be built when the outlayer gets built.\n",
    "- Let's try to build a `MLPBlock` that contains three `Linear` sublayers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        X = self.linear_1(inputs)\n",
    "        X = tf.nn.relu(X)\n",
    "        \n",
    "        X = self.linear_2(X)\n",
    "        X = tf.nn.relu(X)\n",
    "        \n",
    "        return self.linear_3(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPBlock()\n",
    "y = mlp(tf.ones(shape=(3, 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:  6\n",
      "trainable weights:  6\n"
     ]
    }
   ],
   "source": [
    "print(\"weights: \", len(mlp.weights))\n",
    "print(\"trainable weights: \", len(mlp.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The `add_loss()` method\n",
    "- When writing the `call()` method of a layer, you can create loss tensors that you will want to use later, when writing your training loop.\n",
    "- This is doable by calling the `self.add_loss(value)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A layer that creates an activity regularization loss\n",
    "class ActivityRegularizationLayer(keras.layers.Layer):\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super().__init__()\n",
    "        self.rate = rate\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These losses (including those created by any inner layer) can be retrieved via `layer.losses`.\n",
    "- This property is reset at the start of every `__call__()` to the top-level layer, so that `layer.losses`always contains the loss value created during the last forward pass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OuterLayer(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__()\n",
    "        self.activity_reg = ActivityRegularizationLayer(1e-2)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.activity_reg(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = OuterLayer()\n",
    "assert len(layer.losses) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1  # We created one loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer(tf.zeros(1, 1)) # layer.losses gets reset at the start of each __call__\n",
    "assert len(layer.losses) == 1  # This is the loss created during the call above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In addition, the `loss` property also contains regularization losses created for the weights of any inner layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OuterLayerWithKernelRegularizer(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = keras.layers.Dense(30, kernel_regularizer=keras.regularizers.l2(1e-3))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=242, shape=(), dtype=float32, numpy=0.0017028431>]\n"
     ]
    }
   ],
   "source": [
    "layer = OuterLayerWithKernelRegularizer()\n",
    "layer(tf.zeros((1,1)))\n",
    "print(layer.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These losses are meant to be taken into account when writing training loops, like the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer\n",
    "\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the batches of a dataset\n",
    "\n",
    "# for X_batch_train, y_batch_train in train_dataset:\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         logits = layer(X_batch_train) # Get logits fro this minibatch\n",
    "#         loss_value = loss_fn(y_batch_train, logits) # Get loss value for this minibatch\n",
    "#         loss_value += sum(model.losses) # Add extra losses created during this forward pass\n",
    "        \n",
    "#     grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "#     optimizer.apply_gradients(zip(grads, model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These losses also work seamlessly with `fit()`.\n",
    "    - They get automatically summed and added to the main loss, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = keras.Input(shape=(3,))\n",
    "outputs = ActivityRegularizationLayer()(inputs)\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2 samples\n",
      "WARNING:tensorflow:The list of trainable weights is empty. Make sure that you are not setting model.trainable to False before compiling the model.\n",
      "2/2 [==============================] - 0s 57ms/sample - loss: 0.2803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8d03463d90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If there is a loss passed in 'compile', 3 regularization losses get added to it\n",
    "model.compile(optimizer='nadam', loss='mse')\n",
    "model.fit(np.random.random((2, 3)), np.random.random((2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's also possible not to pass any loss in 'compile', since the model already has a loss to minimize, via the\n",
    "# 'add_loss' call during the forward pass\n",
    "\n",
    "# model.compile(optimizer='nadam')\n",
    "# model.fit(np.random.random((2, 3)), np.random.random((2, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The `add_metric()` method\n",
    "- Similarly to `add_loss()`, layers also have an `add_metric()` method for tracking the moving average of a quantity during training.\n",
    "- Consider the following \"logistic endpoint\" layer.\n",
    "    - It takes inputs, predictions, and targets.\n",
    "    - It computes a loss which it tracks via `add_loss()`.\n",
    "    - It computes an accuracy scalar, which it tracks via `add_metric()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticEndpoint(keras.layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.accuracy_fn = keras.metrics.BinaryAccuracy()\n",
    "        \n",
    "    def call(self, targets, logits, sample_weights=None):\n",
    "        # Compute the training-time loss value and add it to the layer using self.add_loss()\n",
    "        loss = self.loss_fn(targets, logits, sample_weights)\n",
    "        self.add_loss(loss)\n",
    "        \n",
    "        # Compute og accuracy as a metric and add it to the layer using self.add_metric()\n",
    "        acc = self.accuracy_fn(targets, logits, sample_weights)\n",
    "        self.add_metric(acc, name='accuracy')\n",
    "        \n",
    "        return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Metrics trakced in this way are accessible via `layer.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer.metrics: [<tensorflow.python.keras.metrics.BinaryAccuracy object at 0x7f8d03463450>]\n",
      "current accuracy value: 1.0\n"
     ]
    }
   ],
   "source": [
    "layer = LogisticEndpoint()\n",
    "\n",
    "targets = tf.ones((2, 2))\n",
    "logits = tf.ones((2, 2))\n",
    "y = layer(targets, logits)\n",
    "\n",
    "print(\"layer.metrics:\", layer.metrics)\n",
    "print(\"current accuracy value:\", float(layer.metrics[0].result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Just like for `add_loss()`, these metrics are tracked by `fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(3,), name='inputs')\n",
    "targets = keras.Input(shape=(10,), name='targets')\n",
    "logits = keras.layers.Dense(10)(inputs)\n",
    "predictions = LogisticEndpoint(name='predictions')(logits, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output predictions missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to predictions.\n"
     ]
    }
   ],
   "source": [
    "model = keras.Model(inputs=[inputs, targets], outputs=predictions)\n",
    "model.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"inputs\": np.random.random((3,3)),\n",
    "    \"targets\": np.random.random((3, 10))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3 samples\n",
      "3/3 [==============================] - 1s 193ms/sample - loss: 0.9309 - binary_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8d02f7e1d0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You can optionally enable serialization on your layers\n",
    "- If you need your custom layers to be serializable as part of a Functional model, you can optionally implement a `get_config()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\"units\": self.units}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'units': 64}\n"
     ]
    }
   ],
   "source": [
    "layer = Linear(64)\n",
    "config = layer.get_config()\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'units': 64}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_layer = Linear.from_config(config)\n",
    "new_layer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that the `__init__()` method of the base `Layer` class takes some keyword arguments, in particular a `name` and a `dtype`.\n",
    "- It's good practice to pass these arguments to the parent class in `__init__()` and to include them in the layer config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units),\n",
    "            initializer='random_normal',\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'linear_8', 'trainable': True, 'dtype': 'float32', 'units': 64}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = Linear(64)\n",
    "config = layer.get_config()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'linear_8', 'trainable': True, 'dtype': 'float32', 'units': 64}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_layer = Linear.from_config(config)\n",
    "new_layer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privileged `training` argument in the `call()` method\n",
    "- Some layers, in particular the `BatchNormalization` layer and the `Dropout` layer, have different behaviors during training and inference.\n",
    "- For such layers, it is a standard practice to expose a `training` (boolean) argument in the `call()` method.\n",
    "- By exposing this argument in `call()`, you enbale the built-in training and evaluation loops (e.g. `fit()`) to correctly use the layer in training and inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDropout(keras.layers.Layer):\n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs, rate=self.rate)\n",
    "        else:\n",
    "            return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The `Model` Class\n",
    "- In general, you will use the `Layer` class to define inner computation blocks, and will use the `Model` class to define the outer model - the object you will train.\n",
    "- The `Model` class has the same API as `Layer`, with the following differences:\n",
    "    - It exposes **built-in training, evaluation, and prediction loops** (`model.fit()`, `model.evaluate()`, `model.predict()`).\n",
    "    - It exposes the **list of its inner layers**, via the `model.layers` property.\n",
    "    - It exposes **saving and serialization APIs** (`save()`, `save_weights()`, etc.).\n",
    "- In general, if you will need to call `fit()` or `save()` on the class you are creating, go with `Model`, otherwise go with `Layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume we've created a ResNetBlock subclass of Layer\n",
    "\n",
    "# class ResNet(tf.keras.Model):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.block_1 = ResNetBlock()\n",
    "#         self.block_2 = ResNetBlock()\n",
    "#         self.global_pool = layers.GlobalAveragePooling2D()\n",
    "#         self.classifier = Dense(num_classes)\n",
    "        \n",
    "#     def call(self, inputs):\n",
    "#         X = self.block_1(inputs)\n",
    "#         X = self.block_2(X)\n",
    "#         X = self.global_pool(X)\n",
    "#         return self.classifier(X)\n",
    "    \n",
    "# resnet = ResNet()\n",
    "# dataset = ...\n",
    "# resnet.fit(dataset, epochs=10)\n",
    "# resnet.save('filepath')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together: an end-to-end example\n",
    "- Here's what you've learned so far:\n",
    "    - A `Layer` encapsulate a state (created in `__init__()` or `build()`) and some computation (defined in `call()`).\n",
    "    - Layers can be recursively nested to create new, bigger computation blocks.\n",
    "    - Layers can create and track losses (typically regularization losses) as well as metrics, via `add_loss()` and `add_metric()`.\n",
    "    - The outer container, the thing you want to train, is a `Model`.\n",
    "    - A `Model` is just like a `Layer`, but with added training and serialization utilities.\n",
    "- Let's put all of these things together into an end-to-end example: we're going to implement a **Variational AutoEncoder (VAE)**, and we'll train it on MNIST digits.\n",
    "    - Our VAE will be a subclass of `Model`, built as nested composition of layers that subclass `Layer`.\n",
    "    - It will feature a regularization loss (KL divergence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(keras.layers.Layer):\n",
    "    # Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(keras.layers.Layer):\n",
    "    # Maps MNIST digits to a triplet (z_mean, z_log_var, z).\n",
    "    \n",
    "    def __init__(self, latent_dim=32, intermediate_dim=64, name='encoder', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.dense_proj = keras.layers.Dense(intermediate_dim, activation='relu')\n",
    "        self.dense_mean = keras.layers.Dense(latent_dim)\n",
    "        self.dense_log_var = keras.layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        X = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(X)\n",
    "        z_log_var = self.dense_log_var(X)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(keras.layers.Layer):\n",
    "    # Converts z, the encoded digit vector, back into a readable digit.\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, name=\"decoder\", **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = keras.layers.Dense(intermediate_dim, activation=\"relu\")\n",
    "        self.dense_output = keras.layers.Dense(original_dim, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(keras.Model):\n",
    "    # Combines the encoder and decoder into an end-to-end model for training\n",
    "    \n",
    "    def __init__(self, original_dim, intermediate_dim=64, latent_dim=32, name='autoencoder', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        \n",
    "        # Add KL divergence regularization loss\n",
    "        kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "        \n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VariationalAutoEncoder(784, 64, 32)\n",
    "vae.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=keras.optimizers.Adam(learning_rate=1e-3))\n",
    "# vae.fit(X_train, y_train, epochs=2, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
