{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- **Masking** is a way to tell sequence-processing layers that certain timesteps in an inputs are missing and thus should be skipped when processing the data.\n",
    "- **Padding** is a special form of masking where the masked steps are at the start or the beggining of the sequence.\n",
    "    - Padding comes from the need to encode sequence data into contiguous batches: in order to make all sequences in a batch fit a given standard length, it is necessary to pad or truncate some sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding sequence data\n",
    "- When processing sequence data, it is very common for individual samples to have different lengths.\n",
    "- Consider the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', 'world', '!'],\n",
       " ['How', 'are', 'you', 'doing', 'today'],\n",
       " ['The', 'weather', 'will', 'be', 'nice', 'tomorrow']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    [\"Hello\", \"world\", \"!\"],\n",
    "    [\"How\", \"are\", \"you\", \"doing\", \"today\"],\n",
    "    [\"The\", \"weather\", \"will\", \"be\", \"nice\", \"tomorrow\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After vocabulary lookup, the data might be vectorized as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[71, 1331, 4231], [73, 8, 3215, 55, 927], [83, 91, 1, 645, 1253, 927]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "  [71, 1331, 4231],\n",
    "  [73, 8, 3215, 55, 927],\n",
    "  [83, 91, 1, 645, 1253, 927]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data is a nested list with individual samples with length 3, 5, and 6.\n",
    "- Since the input data for a deep learning model must be a single tensor, samples that are shorter than the longest item need to be **padded with some placeholder value**.\n",
    "- Alternatively, one might also **truncate long samples before padding short samples**.\n",
    "- Keras provides a utility function to truncate and pad Python lists to a common length: `tf.keras.preprocessing.sequence.pad_sequence`.\n",
    "    - By default, the function will pad with 0s, but the placeholder value can be configured using the `value` parameter.\n",
    "    - Note that you could `pre` padding (at the beginning) or `post` padding (at the end).\n",
    "        - We recommend using `post` padding when working with RNN layers in order to use the CuDNN of the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "    [711, 632, 71],\n",
    "    [73, 8, 3215, 55, 927],\n",
    "    [83, 91, 1, 645, 1253, 927],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 711,  632,   71,    0,    0,    0],\n",
       "       [  73,    8, 3215,   55,  927,    0],\n",
       "       [  83,   91,    1,  645, 1253,  927]], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masking\n",
    "- Now that all samples have a uniform length, the model must be informed that some part of the data is actually padded and should be ignored.\n",
    "- This machanism is **masking**.\n",
    "- There are three ways to introduce masks in Keras models:\n",
    "    - Add a `keras.layers.Masking` layer\n",
    "    - Configure a `keras.layers.Embedding` layer with `mask_zero=True`\n",
    "        - The `keras.layers.Embedding` layer turns positive integers (indexes) into dense vectors of fixed size.\n",
    "    - Pass a `mask` argument manually when calling layers that support this argument (e.g. RNN layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking-generating layers: `Embedding` and `Masking`\n",
    "- Under the hood, these layers will create a mask tensor (2D tensor with shape `(batch, sequence_length)`), and attach it to the tensor output returned by the `Masking` or `Embedding` layer.\n",
    "    - Each individual `False` entry indicates that the corresponding timestep should be ignored during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ True  True  True False False False]\n",
      " [ True  True  True  True  True False]\n",
      " [ True  True  True  True  True  True]], shape=(3, 6), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n",
    "masked_output = embedding(padded_inputs)\n",
    "\n",
    "print(masked_output._keras_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ True  True  True False False False]\n",
      " [ True  True  True  True  True False]\n",
      " [ True  True  True  True  True  True]], shape=(3, 6), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "masking_layer = layers.Masking()\n",
    "\n",
    "# Simulate the embedding lookup by expanding the 2D input to 3D with embedding dimension of 10\n",
    "unmasked_embedding = tf.cast(\n",
    "    tf.tile(tf.expand_dims(padded_inputs, axis=-1), [1,1,10]), tf.float32\n",
    ")\n",
    "\n",
    "masked_embedding = masking_layer(unmasked_embedding)\n",
    "print(masked_embedding._keras_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask propagation in the Functional API and Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential API\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True),\n",
    "    layers.LSTM(32)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API\n",
    "inputs = keras.Input(shape=(None, ), dtype='int32')\n",
    "X = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs)\n",
    "outputs = layers.LSTM(32)(X)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing mask tensors directly to layers\n",
    "- Layers that can handle masks (such as the `LSTM` layer) have a `mask` argument in their `__call__` method.\n",
    "- Meanwhile, layers that produce a mask (e.g. `Embedding`) expose a `compute_mask(input, previous_mask)` method which you can call.\n",
    "- Thus, you can pass the output of the **`compute_mask()` method of a mask-producing layer** to the **`__call__` method of a mask-consuming layer**, like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n",
    "        self.lstm = layers.LSTM(32)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        X = self.embedding(inputs)\n",
    "        mask = self.embedding.compute_mask(inputs) # Note that you can also prepare a mask tensor manually, which is a boolean tensor with the right shape\n",
    "        output = self.lstm(X, mask=mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4712, shape=(32, 32), dtype=float32, numpy=\n",
       "array([[ 0.0039643 , -0.00655454, -0.00223116, ..., -0.00368348,\n",
       "         0.00161009, -0.00469382],\n",
       "       [-0.00226298,  0.00542886,  0.00200883, ..., -0.00303189,\n",
       "         0.00095562, -0.0027541 ],\n",
       "       [ 0.00613192, -0.00525401,  0.00232902, ...,  0.00764465,\n",
       "         0.00270565,  0.00929067],\n",
       "       ...,\n",
       "       [ 0.00265467, -0.0041955 ,  0.00143585, ..., -0.00253374,\n",
       "         0.00506226,  0.00651153],\n",
       "       [-0.00052386, -0.00336463, -0.00240109, ...,  0.00329279,\n",
       "        -0.00707992, -0.00113707],\n",
       "       [ 0.00523299, -0.00444659,  0.00104043, ..., -0.00032395,\n",
       "        -0.00407169,  0.00483833]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = MyLayer()\n",
    "\n",
    "X = np.random.random((32, 10)) * 100\n",
    "X = X.astype(\"int32\")\n",
    "layer(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting masking in your custom layers\n",
    "- Sometimes, you may need to write layers that generate a mask (like `Embedding`), or layers that need to modify the current mask.\n",
    "    - For instance, any layer that produces a tensor with a different time dimension than its input, such as a Concatenate layer that concatenates on the time dimension, will need to modify the current mask so that downstream layers will be able to properly take masked timesteps into account.\n",
    "- To do this, your layer should implement the `layer.compute_mask()` method, which produces a new mask given the input and the current mask.\n",
    "- Here is an example of a TemporalSplit layer that needs to modify the current mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This layer splits the input tensor into 2 tensors along the time dimension\n",
    "class TemporalSplit(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.split(inputs, 2, axis=1)\n",
    "    \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if mask is None:\n",
    "            return None\n",
    "        else:\n",
    "            return tf.split(mask, 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]], shape=(3, 3), dtype=bool)\n",
      "tf.Tensor(\n",
      "[[False False False]\n",
      " [ True  True False]\n",
      " [ True  True  True]], shape=(3, 3), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "first_half, second_half = TemporalSplit()(masked_embedding)\n",
    "print(first_half._keras_mask)\n",
    "print(second_half._keras_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here's another example of a `CustomEmbedding` layer that is capable of generating a mask from input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim, mask_zero=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.mask_zero = mask_zero\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.embeddings = self.add_weight(\n",
    "            shape=(self.input_dim, self.output_dim),\n",
    "            initializer='random_normal',\n",
    "            dtype='float32'\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.nn.embedding_lookup(self.embeddings, inputs)\n",
    "    \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if not self.mask_zero:\n",
    "            return None\n",
    "        else:\n",
    "            return tf.not_equal(inputs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ True False  True  True  True  True  True  True False  True]\n",
      " [ True False False False  True  True  True  True  True  True]\n",
      " [ True  True  True  True False  True  True  True  True  True]], shape=(3, 10), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "layer = CustomEmbedding(10, 32, mask_zero=True)\n",
    "\n",
    "X = np.random.random((3, 10)) * 9\n",
    "X = X.astype(\"int32\")\n",
    "\n",
    "y = layer(X)\n",
    "mask = layer.compute_mask(X)\n",
    "\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opting-in to mask propagation on compatible layers\n",
    "- Most layers don't modify the time dimension, so don't need to modify the current mask. \n",
    "- However, they may still want to be able to propagate the current mask, unchanged, to the next layer. This is an **opt-in behavior**. \n",
    "    - By default, a custom layer will destroy the current mask (since the framework has no way to tell whether propagating the mask is safe to do).\n",
    "- If you have a custom layer that does not modify the time dimension, and if you want it to be able to propagate the current input mask, you should set `self.supports_masking = True` in the layer constructor. \n",
    "    - In this case, the default behavior of `compute_mask()` is just pass the current mask through.\n",
    "- Here's an example of a layer that is whitelisted for mask propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyActivation(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Signal that the layer is safe for mask propagation\n",
    "        self.supports_masking = True\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.nn.relu(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You can now use this custom layer in-between a mask-generating layer (like `Embedding`) and a mask-consuming layer (like `LSTM`), and it will **pass the mask along** so that it reachs the mask-consuming layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask found Tensor(\"embedding_4/NotEqual:0\", shape=(None, None), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype='int32')\n",
    "X = layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)(inputs)\n",
    "X = MyActivation()(X) # Will pass the mask along\n",
    "print(\"Mask found\", X._keras_mask)\n",
    "outputs = layers.LSTM(32)(X) # Will receive the mask\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing layers that need mask information\n",
    "- Some layers are **mask consumers**: they accept a mask argument in call and use it to determine whether to skip certain time steps.\n",
    "- To write such a layer, you can simply add a `mask=None` argument in your `call` signature. \n",
    "    - The mask associated with the inputs will be passed to your layer whenever it is available.\n",
    "- Here's a simple example below: a layer that computes a softmax over the time dimension (axis 1) of an input sequence, while discarding masked timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalSoftmax(keras.layers.Layer):\n",
    "    def call(self, inputs, mask=None):\n",
    "        broad_float_mask = tf.expand_dims(tf.cast(mask, 'float32'), -1)\n",
    "        inputs_exp = tf.exp(inputs) * broad_float_mask\n",
    "        inputs_sum = tf.reduce_sum(inputs * broad_float_mask, axis=1, keepdims=True)\n",
    "        return inputs_exp / inputs_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "X = layers.Embedding(input_dim=10, output_dim=32, mask_zero=True)(inputs)\n",
    "X = layers.Dense(1)(X)\n",
    "outputs = TemporalSoftmax()(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs, outputs)\n",
    "y = model(np.random.randint(0, 10, size=(32, 100)), np.random.random((32, 100, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=6936, shape=(32, 100, 1), dtype=float32, numpy=\n",
       "array([[[-0.5138908 ],\n",
       "        [-0.52876   ],\n",
       "        [-0.54227686],\n",
       "        ...,\n",
       "        [-0.54227686],\n",
       "        [-0.5360632 ],\n",
       "        [-0.5231054 ]],\n",
       "\n",
       "       [[-0.33242685],\n",
       "        [-0.34676975],\n",
       "        [-0.33242685],\n",
       "        ...,\n",
       "        [-0.33542195],\n",
       "        [-0.        ],\n",
       "        [-0.32429668]],\n",
       "\n",
       "       [[-0.49944207],\n",
       "        [-0.47864464],\n",
       "        [-0.4950649 ],\n",
       "        ...,\n",
       "        [-0.47864464],\n",
       "        [-0.5048409 ],\n",
       "        [-0.50174344]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.385618  ],\n",
       "        [-0.37282786],\n",
       "        [-0.37870866],\n",
       "        ...,\n",
       "        [-0.39866406],\n",
       "        [-0.37870866],\n",
       "        [-0.38902748]],\n",
       "\n",
       "       [[-0.61954373],\n",
       "        [-0.626725  ],\n",
       "        [-0.        ],\n",
       "        ...,\n",
       "        [-0.61954373],\n",
       "        [-0.61954373],\n",
       "        [-0.6073538 ]],\n",
       "\n",
       "       [[-0.50378186],\n",
       "        [-0.        ],\n",
       "        [-0.5166602 ],\n",
       "        ...,\n",
       "        [-0.5166602 ],\n",
       "        [-0.51074004],\n",
       "        [-0.47764057]]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- This is all you need to know about padding & masking in Keras.\n",
    "    - **Masking** is how layers are able to know when to skip/ignore certain timesteps in sequence inputs.\n",
    "    - Some layers are **mask-generators**.\n",
    "        - `Embedding` can generate a mask from input values (if `mask_zero=True`), and so can the `Mask` layer.\n",
    "    - Some layers are **mask-consumers**.\n",
    "        - They expose a `mask` argument in their `__call__` method (e.g. RNN layers).\n",
    "    - In the Functional API and Sequential API, mask information is propagated automatically.\n",
    "    - When using layers in a standalone way, you can pass the `mask` arguments to layers manually.\n",
    "    - You can easily write layers that modify the current mask, that generate a new mask, or that consume the mask associated with the inputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
