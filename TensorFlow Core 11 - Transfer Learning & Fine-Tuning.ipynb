{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "- **Transfer learning** consists of taking features learned on one problem, and leveraging them on a new, similar problem. \n",
    "    - For instance, features from a model that has learned to identify racoons may be useful to kick-start a model meant to identify tanukis.\n",
    "- Transfer learning is usually done for tasks where your dataset has too little data to train a full-scale model from scratch.\n",
    "- The most common incarnation of transfer learning in the context of deep learning is the following worfklow:\n",
    "    1. Take layers from a previously trained model\n",
    "    2. Freeze them, so as to avoid destroying any of the information they contain during future training rounds\n",
    "    3. Add some new, trainable layers on top of the frozen layers\n",
    "        - They will learn to turn the old features into predictions on a new dataset.\n",
    "    4. Train the new layers on your dataset\n",
    "- A last, optional step, is **fine-tuning**, which consists of unfreezing the entire model you obtained above (or part of it), and re-training it on the new data with a very low learning rate. \n",
    "    - This can potentially achieve meaningful improvements, by incrementally adapting the pretrained features to the new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freezing layers: understanding the `trainable` attribute\n",
    "- Layers & models have three weight attributes:\n",
    "    - `weights` is the list of all weights variables of the layer.\n",
    "    - `trainable_weights` is the list of those that are meant to be updated (via gradient descent) to minimize the loss during training.\n",
    "    - `non_trainable_weights` is the list of those that aren't meant to be trained.\n",
    "        - Typically, they are updated by the model during the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 2\n",
      "trainable_weights: 2\n",
      "non_trainable_weights: 0\n"
     ]
    }
   ],
   "source": [
    "layer = keras.layers.Dense(3)\n",
    "layer.build((None, 4)) # Create the weights\n",
    "\n",
    "print(\"weights:\", len(layer.weights))\n",
    "print(\"trainable_weights:\", len(layer.trainable_weights))\n",
    "print(\"non_trainable_weights:\", len(layer.non_trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In general, all weights are trainable weights. \n",
    "- The only built-in layer that has non-trainable weights is the `BatchNormalization` layer. \n",
    "    - It uses non-trainable weights to keep track of the mean and variance of its inputs during training. \n",
    "- To learn how to use non-trainable weights in your own custom layers, see the guide to writing new layers from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 4\n",
      "trainable_weights: 2\n",
      "non_trainable_weights: 2\n"
     ]
    }
   ],
   "source": [
    "layer = keras.layers.BatchNormalization()\n",
    "layer.build((None, 4)) # Create the weights\n",
    "\n",
    "print(\"weights:\", len(layer.weights))\n",
    "print(\"trainable_weights:\", len(layer.trainable_weights))\n",
    "print(\"non_trainable_weights:\", len(layer.non_trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Layers & models also feature a boolean attribute `trainable`. \n",
    "- Setting `layer.trainable = False` moves all the layer's weights from trainable to non-trainable. \n",
    "    - This is called **\"freezing\" the layer**: the state of a frozen layer won't be updated during training (either when training with `fit()` or when training with any custom loop that relies on `trainable_weights` to apply gradient updates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 2\n",
      "trainable_weights: 0\n",
      "non_trainable_weights: 2\n"
     ]
    }
   ],
   "source": [
    "layer = keras.layers.Dense(3)\n",
    "layer.build((None, 4)) # Create the weights\n",
    "layer.trainable = False # Freeze the layer\n",
    "\n",
    "print(\"weights:\", len(layer.weights))\n",
    "print(\"trainable_weights:\", len(layer.trainable_weights))\n",
    "print(\"non_trainable_weights:\", len(layer.non_trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When a trainable weight becomes non-trainable, its value is no longer updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a model with 2 layers\n",
    "layer1 = keras.layers.Dense(3, activation='relu')\n",
    "layer2 = keras.layers.Dense(3, activation='sigmoid')\n",
    "model = keras.Sequential([\n",
    "    keras.Input(shape=(3,)),\n",
    "    layer1,\n",
    "    layer2\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the first layer\n",
    "layer1.trainable = False\n",
    "\n",
    "# Keep a copy of the weights of layer1 for later reference\n",
    "initial_layer1_weights_values = layer1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2 samples\n",
      "2/2 [==============================] - 1s 301ms/sample - loss: 0.0575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc99e5de910>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(np.random.random((2,3)), np.random.random((2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the weights of layer1 have not changed during training\n",
    "final_layer1_weights_values = layer1.get_weights()\n",
    "\n",
    "np.testing.assert_allclose(initial_layer1_weights_values[0], final_layer1_weights_values[0])\n",
    "np.testing.assert_allclose(initial_layer1_weights_values[1], final_layer1_weights_values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: do not confuse the `layer.trainable` attribute with the argument `training` in `layer.__call__()`, which controls whether the layer should run its forward pass in reference mode or training mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive setting of the `trainable` attribute\n",
    "- If you set `trainable = False` on a model or on any layer that has sublayers, all children layers become non-trainable as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_model = keras.Sequential([\n",
    "    keras.Input(shape=(3,)),\n",
    "    keras.layers.Dense(3, activation='relu'),\n",
    "    keras.layers.Dense(3, activation='relu')\n",
    "])\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.Input(shape=(3,)),\n",
    "    inner_model,\n",
    "    keras.layers.Dense(3, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = False # Freeze the outer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert inner_model.trainable == False # All layers in `model` are now frozen\n",
    "assert inner_model.layers[0].trainable == False # `trainable` is propagated recurvisely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The typical transfer-learning workflow\n",
    "- This leads us to how a typical transfer learning workflow can be implemented in Keras:\n",
    "    1. Instantiate a base model and load pre-trained weights into it\n",
    "    2. Freeze all layers in the base model by setting `trainable = False`\n",
    "    3. Create a new model on top of the output of one (or several) layers from the base model\n",
    "    4. Train your new model on your new dataset\n",
    "- Note that an alternative, more lightweight workflow could also be:\n",
    "    1. Instantiate a base model and load pre-trained weights into it\n",
    "    2. Run your new dataset through it and record the output of one (or several) layers from the base model \n",
    "        - This is called feature extraction.\n",
    "    3. Use that output as input data for a new, smaller model\n",
    "- A key advantage of that second workflow is that you only run the base model once one your data, rather than once per epoch of training. So it's a lot faster & cheaper.\n",
    "- An issue with that second workflow, though, is that it doesn't allow you to dynamically modify the input data of your new model during training, which is required when doing data augmentation, for instance. \n",
    "- Transfer learning is typically used for tasks when your new dataset has too little data to train a full-scale model from scratch, and in such scenarios data augmentation is very important. So in what follows, we will focus on the first workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a base model with pre-trained weights\n",
    "base_model = keras.applications.Xception(\n",
    "    weights='imagenet', # Load weights pre-trained on ImageNet\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False # Do not include the ImageNet classifier at the top\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the base model\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model on top\n",
    "inputs = keras.Input(shape=(150, 150, 3))\n",
    "\n",
    "# We make sure that the base_model is running in inference mode here by passing `training=False`.\n",
    "X = base_model(inputs, training=False)\n",
    "\n",
    "X = keras.layers.GlobalAveragePooling2D()(X)\n",
    "outputs = keras.layers.Dense(1)(X)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on new data\n",
    "model.compile(optimizer=\"adam\", \n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "# model.fit(new_dataset, epoch=20, callbacks=..., validation_data=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning\n",
    "- Once your model has converged on the new data, you can try to unfreeze all or part of the base model and retrain the whole model end-to-end with a very low learning rate.\n",
    "    - This is an optional last step that can potentially give you incremental improvements. \n",
    "    - It could also potentially lead to quick overfitting -- keep that in mind.\n",
    "- It is critical to only do this step after the model with frozen layers has been trained to convergence. \n",
    "    - If you mix randomly-initialized trainable layers with trainable layers that hold pre-trained features, the randomly-initialized layers will cause very large gradient updates during training, which will destroy your pre-trained features.\n",
    "- It's also critical to use a very low learning rate at this stage, because you are training a much larger model than in the first round of training, on a dataset that is typically very small. \n",
    "    - As a result, you are at risk of overfitting very quickly if you apply large weight updates. \n",
    "    - Here, you only want to readapt the pretrained weights in an incremental way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Remember to recompile model after make changes to the `trainable` attribute of any inner layer\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-5),  # Very low learning rate\n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "# Train end-to-end\n",
    "# Be careful to stop before overfitting\n",
    "# model.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note about `compile()` and `trainable`**\n",
    "- Calling `compile()` on a model is meant to \"freeze\" the behavior of that model. \n",
    "    - This implies that the `trainable` attribute values at the time the model is compiled should be preserved throughout the lifetime of that model, until `compile` is called again. \n",
    "    - Hence, if you change any trainable value, make sure to call `compile()` again on your model for your changes to be taken into account.\n",
    "\n",
    "**Important notes about `BatchNormalization` layer**\n",
    "- Many image models contain `BatchNormalization` layers. \n",
    "    - That layer is a special case on every imaginable count.\n",
    "- Here are a few things to keep in mind.\n",
    "    - `BatchNormalization` contains 2 non-trainable weights that get updated during training. These are the variables tracking the mean and variance of the inputs.\n",
    "    - When you set `bn_layer.trainable = False`, the `BatchNormalization` layer will run in inference mode, and will not update its mean & variance statistics. \n",
    "        - This is not the case for other layers in general, as weight trainability & inference/training modes are two orthogonal concepts. But the two are tied in the case of the `BatchNormalization` layer.\n",
    "    - When you unfreeze a model that contains `BatchNormalization` layers in order to do fine-tuning, you should keep the `BatchNormalization` layers in inference mode by passing `training=False` when calling the base model. \n",
    "        - Otherwise the updates applied to the non-trainable weights will suddenly destroy what the model the model has learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning & fine-tuning with a custom training loop\n",
    "- If instead of `fit()` you are using your own low-level training loop, the workflow stays essentially the same.\n",
    "- You should be careful to only take into account the list `model.trainable_weights` when applying gradient updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base model\n",
    "base_model = keras.applications.Xception(\n",
    "    weights='imagenet',\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False)\n",
    "# Freeze base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create new model on top.\n",
    "inputs = keras.Input(shape=(150, 150, 3))\n",
    "X = base_model(inputs, training=False)\n",
    "X = keras.layers.GlobalAveragePooling2D()(X)\n",
    "outputs = keras.layers.Dense(1)(X)\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Iterate over the batches of a dataset\n",
    "# for inputs, targets in new_dataset:\n",
    "#     # Open a GradientTape\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         predictions = model(inputs) # Forward pass\n",
    "#         loss_value = loss_fn(targets, predictions) # Compute the loss value for this batch\n",
    "        \n",
    "#     # Get gradients of loss w.r.t. the trainable_weights\n",
    "#     gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "    \n",
    "#     # Update the weights of the model\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An end-to-end example: fine-tuning an image classification model on cats vs. dogs\n",
    "- So many things have been updated, and the code from the guidance page no longer works.\n",
    "- To view the code, visit:\n",
    "https://www.tensorflow.org/guide/keras/transfer_learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
